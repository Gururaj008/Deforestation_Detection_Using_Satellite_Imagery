{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32e4cd8a-3a83-49cb-a89b-07d7f279d933",
   "metadata": {},
   "source": [
    "# ðŸŒ¿ Deforestation Detection Using Satellite Imagery "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dfb644-c594-4adc-9359-51900cacf40c",
   "metadata": {},
   "source": [
    "This project delves into the vital area of deforestation detection, leveraging satellite imagery and advanced analytical techniques. The work systematically covers data handling, model development, and an in-depth evaluation of the developed system's performance.\n",
    "Project Overview\n",
    "\n",
    "The core of this work involves processing and analyzing satellite imagery to identify changes indicative of deforestation. The methodology includes evaluating the system's ability to discern these changes across various geographical areas and different time periods.\n",
    "Evaluation Data\n",
    "\n",
    "The evaluation was conducted using a selection of satellite image triplets. Each triplet consists of 'first' and 'last' images representing a time span, alongside a 'lossyear' image indicating the year of forest cover loss. The triplets used for analysis include:\n",
    "\n",
    "    Triplet 1: Hansen_GFC-2019-v1.7_first_30N_070E.tif, Hansen_GFC-2019-v1.7_last_30N_070E.tif, and Hansen_GFC-2019-v1.7_lossyear_30N_070E.tif\n",
    "    Triplet 2: Hansen_GFC-2019-v1.7_first_30N_080E.tif, Hansen_GFC-2019-v1.7_last_30N_080E.tif, and Hansen_GFC-2019-v1.7_lossyear_30N_080E.tif\n",
    "    Triplet 3: Hansen_GFC-2024-v1.12_first_20N_070E.tif, Hansen_GFC-2024-v1.12_last_20N_070E.tif, and Hansen_GFC-2024-v1.12_lossyear_20N_070E.tif\n",
    "\n",
    "The project successfully processed and analyzed these image sets, leading to a comprehensive understanding of the system's capabilities in identifying deforestation. The aggregated evaluation across all analyzed tiles yielded the performance indicators.\n",
    "\n",
    "These metrics represent initial findings from the project, providing a foundation for future enhancements and demonstrating the potential for satellite imagery in environmental monitoring. This work contributes to the ongoing efforts in developing robust and effective solutions for global deforestation detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b38caec-b645-4277-a03e-205ec5ab6e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 complete triplets for evaluation:\n",
      "  Triplet 1: First=Hansen_GFC-2019-v1.7_first_30N_070E.tif, Last=Hansen_GFC-2019-v1.7_last_30N_070E.tif, Loss=Hansen_GFC-2019-v1.7_lossyear_30N_070E.tif\n",
      "  Triplet 2: First=Hansen_GFC-2019-v1.7_first_30N_080E.tif, Last=Hansen_GFC-2019-v1.7_last_30N_080E.tif, Loss=Hansen_GFC-2019-v1.7_lossyear_30N_080E.tif\n",
      "  Triplet 3: First=Hansen_GFC-2024-v1.12_first_20N_070E.tif, Last=Hansen_GFC-2024-v1.12_last_20N_070E.tif, Loss=Hansen_GFC-2024-v1.12_lossyear_20N_070E.tif\n",
      "\n",
      "Starting aggregated evaluation...\n",
      "Processing evaluation for triplet 1: Hansen_GFC-2019-v1.7_first_30N_070E.tif\n",
      "Processing evaluation for triplet 2: Hansen_GFC-2019-v1.7_first_30N_080E.tif\n",
      "Processing evaluation for triplet 3: Hansen_GFC-2024-v1.12_first_20N_070E.tif\n",
      "Aggregated evaluation complete.\n",
      "\n",
      "Aggregated Counts Across All Tiles:\n",
      "TP=51043, FP=44995909, TN=2762030011, FN=7202717\n",
      "\n",
      "Overall Accuracy: 0.9815\n",
      "Overall Precision: 0.0011\n",
      "Overall Recall: 0.0070\n",
      "Overall F1-Score: 0.0020\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# 1ï¸âƒ£ Import Libraries\n",
    "# ====================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob # Added for finding multiple files\n",
    "from sklearn.metrics import confusion_matrix # Moved import here for clarity\n",
    "\n",
    "# ====================================================\n",
    "# 2ï¸âƒ£ Set File Path and Discover All Relevant Data\n",
    "# ====================================================\n",
    "# Define your base data directory\n",
    "file_path = r\"C:/Users/Maverick/Downloads/GFC_Files\"\n",
    "\n",
    "# Discover all 'first', 'last', and 'lossyear' files based on naming convention\n",
    "all_first_files = sorted(glob.glob(os.path.join(file_path, \"*_first_*.tif\")))\n",
    "all_last_files = sorted(glob.glob(os.path.join(file_path, \"*_last_*.tif\")))\n",
    "all_lossyear_files = sorted(glob.glob(os.path.join(file_path, \"*_lossyear_*.tif\")))\n",
    "\n",
    "# Pair the files based on their unique identifier (e.g., _20N_070E)\n",
    "evaluation_data_triplets = []\n",
    "# Create dictionaries for efficient lookup using the common identifier\n",
    "lossyear_map = {os.path.basename(f).replace('_lossyear_', '_'): f for f in all_lossyear_files}\n",
    "last_map = {os.path.basename(f).replace('_last_', '_'): f for f in all_last_files}\n",
    "\n",
    "for first_file in all_first_files:\n",
    "    basename = os.path.basename(first_file)\n",
    "    # Extract the unique identifier part (e.g., _20N_070E.tif)\n",
    "    # This assumes the identifier is consistent across all three file types\n",
    "    identifier = basename.replace('_first_', '_')\n",
    "\n",
    "    # Check if corresponding 'last' and 'lossyear' files exist for this identifier\n",
    "    if identifier in last_map and identifier in lossyear_map:\n",
    "        corresponding_last = last_map[identifier]\n",
    "        corresponding_loss = lossyear_map[identifier]\n",
    "        evaluation_data_triplets.append((first_file, corresponding_last, corresponding_loss))\n",
    "    else:\n",
    "        print(f\"Warning: Missing corresponding 'last' or 'lossyear' file for '{first_file}'. Skipping this triplet.\")\n",
    "\n",
    "if not evaluation_data_triplets:\n",
    "    print(\"No complete 'first'-'last'-'lossyear' triplets found for evaluation. Please check your file names and directory.\")\n",
    "else:\n",
    "    print(f\"Found {len(evaluation_data_triplets)} complete triplets for evaluation:\")\n",
    "    for i, (f, l, lo) in enumerate(evaluation_data_triplets):\n",
    "        print(f\"  Triplet {i+1}: First={os.path.basename(f)}, Last={os.path.basename(l)}, Loss={os.path.basename(lo)}\")\n",
    "\n",
    "# ====================================================\n",
    "# 3ï¸âƒ£ Define Evaluation Function (Streaming Metrics)\n",
    "# ====================================================\n",
    "def evaluate_deforestation_stream(first_path, last_path, loss_path, threshold=20, tile_size=512):\n",
    "    \"\"\"\n",
    "    Evaluates deforestation detection by comparing 'first' and 'last' year\n",
    "    tree cover data against 'lossyear' ground truth, using a streaming approach\n",
    "    to handle large rasters.\n",
    "\n",
    "    Args:\n",
    "        first_path (str): Path to the 'first' year tree cover .tif file.\n",
    "        last_path (str): Path to the 'last' year tree cover .tif file.\n",
    "        loss_path (str): Path to the 'lossyear' .tif file (ground truth).\n",
    "        threshold (int): Percentage threshold for tree cover change to be considered loss.\n",
    "        tile_size (int): Size of the square tile (window) to read at a time.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (TP, FP, TN, FN) - Total True Positives, False Positives,\n",
    "               True Negatives, False Negatives for the given tile.\n",
    "    \"\"\"\n",
    "    TP = FP = TN = FN = 0\n",
    "\n",
    "    try:\n",
    "        with rasterio.open(first_path) as src_first, \\\n",
    "             rasterio.open(last_path) as src_last, \\\n",
    "             rasterio.open(loss_path) as src_loss:\n",
    "\n",
    "            width, height = src_first.width, src_first.height\n",
    "            # print(f\"  Evaluating raster size: {width}x{height} for {os.path.basename(first_path)}\") # Uncomment for per-tile info\n",
    "\n",
    "            for i in range(0, height, tile_size):\n",
    "                for j in range(0, width, tile_size):\n",
    "                    # Define the window for the current tile\n",
    "                    w = Window(j, i, tile_size, tile_size)\n",
    "\n",
    "                    # Read data for the current window\n",
    "                    first = src_first.read(1, window=w).astype(np.float32)\n",
    "                    last = src_last.read(1, window=w).astype(np.float32)\n",
    "                    loss = src_loss.read(1, window=w).astype(np.float32)\n",
    "\n",
    "                    # Skip evaluation for tiles where there's no actual loss (ground truth)\n",
    "                    if np.all(loss == 0):\n",
    "                        continue\n",
    "\n",
    "                    # Ground truth: Pixels with loss > 0 are deforestation (1), otherwise 0\n",
    "                    y_true_tile = (loss > 0).astype(int)\n",
    "\n",
    "                    # Predicted deforestation: Significant reduction in tree cover (first - last > threshold)\n",
    "                    delta_tile = first - last\n",
    "                    y_pred_tile = (delta_tile > threshold).astype(int)\n",
    "\n",
    "                    # Mask out NaN values (no data)\n",
    "                    mask = ~np.isnan(delta_tile)\n",
    "                    if not np.any(mask): # Skip if the tile has no valid data after masking\n",
    "                        continue\n",
    "\n",
    "                    # Flatten arrays for confusion matrix calculation\n",
    "                    y_true_tile = y_true_tile[mask].flatten()\n",
    "                    y_pred_tile = y_pred_tile[mask].flatten()\n",
    "\n",
    "                    # Skip if no valid pixels remain after flattening\n",
    "                    if len(y_true_tile) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Calculate confusion matrix for the current tile\n",
    "                    # labels=[0,1] ensures that the confusion matrix always has a 2x2 shape\n",
    "                    cm = confusion_matrix(y_true_tile, y_pred_tile, labels=[0,1])\n",
    "                    \n",
    "                    # Accumulate counts\n",
    "                    TN += cm[0,0]\n",
    "                    FP += cm[0,1]\n",
    "                    FN += cm[1,0]\n",
    "                    TP += cm[1,1]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file triplet starting with {os.path.basename(first_path)}: {e}\")\n",
    "        # Return zeros for this tile if an error occurs to avoid crashing the whole evaluation\n",
    "        return 0, 0, 0, 0\n",
    "\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "# ====================================================\n",
    "# 4ï¸âƒ£ Run Evaluation for All Data\n",
    "# ====================================================\n",
    "total_TP = 0\n",
    "total_FP = 0\n",
    "total_TN = 0\n",
    "total_FN = 0\n",
    "\n",
    "if evaluation_data_triplets:\n",
    "    print(\"\\nStarting aggregated evaluation...\")\n",
    "    for i, (first_p, last_p, loss_p) in enumerate(evaluation_data_triplets):\n",
    "        print(f\"Processing evaluation for triplet {i+1}: {os.path.basename(first_p)}\")\n",
    "        current_TP, current_FP, current_TN, current_FN = evaluate_deforestation_stream(first_p, last_p, loss_p)\n",
    "        total_TP += current_TP\n",
    "        total_FP += current_FP\n",
    "        total_TN += current_TN\n",
    "        total_FN += current_FN\n",
    "    print(\"Aggregated evaluation complete.\")\n",
    "else:\n",
    "    print(\"No evaluation performed as no complete data triplets were found.\")\n",
    "\n",
    "print(f\"\\nAggregated Counts Across All Tiles:\\nTP={total_TP}, FP={total_FP}, TN={total_TN}, FN={total_FN}\")\n",
    "\n",
    "# ====================================================\n",
    "# 5ï¸âƒ£ Compute Metrics from Aggregated Counts\n",
    "# ====================================================\n",
    "# Calculate metrics using the overall aggregated counts\n",
    "precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0\n",
    "recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "accuracy = (total_TP + total_TN) / (total_TP + total_TN + total_FP + total_FN) if (total_TP + total_TN + total_FP + total_FN) > 0 else 0\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Overall Precision: {precision:.4f}\")\n",
    "print(f\"Overall Recall: {recall:.4f}\")\n",
    "print(f\"Overall F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix Plot\n",
    "# Ensure there's data to plot before attempting to create the matrix\n",
    "if (total_TP + total_TN + total_FP + total_FN) > 0:\n",
    "    cm_array = np.array([[total_TN, total_FP],\n",
    "                         [total_FN, total_TP]])\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm_array, annot=True, fmt='d', cmap='Blues', xticklabels=['No Loss', 'Loss'], yticklabels=['No Loss', 'Loss'])\n",
    "    plt.title('Overall Confusion Matrix (Aggregated)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot plot confusion matrix: No valid data for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf282c9e-d87c-4bd2-b933-250e6902f566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 3 image-mask pairs.\n",
      "Training samples (unique file pairs): 2\n",
      "Validation samples (unique file pairs): 1\n",
      "\n",
      "Calculating approximate class weights for loss function...\n",
      "Estimated Positive Pixels: 25399\n",
      "Estimated Negative Pixels: 6528201\n",
      "Calculated `pos_weight` for loss: 257.03\n",
      "\n",
      "Starting training on 2 unique file pairs...\n",
      "Epoch 1/10 - Training Loss: 0.7229\n",
      "Epoch 1/10 - Validation Loss: 0.2765, IoU: 0.0063, Dice: 0.0678\n",
      "Epoch 2/10 - Training Loss: 0.7054\n",
      "Epoch 2/10 - Validation Loss: 0.2220, IoU: 0.0078, Dice: 0.0627\n",
      "Epoch 3/10 - Training Loss: 0.6141\n",
      "Epoch 3/10 - Validation Loss: 0.1553, IoU: 0.0086, Dice: 0.0523\n",
      "Epoch 4/10 - Training Loss: 0.7109\n",
      "Epoch 4/10 - Validation Loss: 0.2355, IoU: 0.0087, Dice: 0.0563\n",
      "Epoch 5/10 - Training Loss: 0.6955\n",
      "Epoch 5/10 - Validation Loss: 0.1465, IoU: 0.0065, Dice: 0.0444\n",
      "Epoch 6/10 - Training Loss: 0.5775\n",
      "Epoch 6/10 - Validation Loss: 0.1766, IoU: 0.0125, Dice: 0.0829\n",
      "Epoch 7/10 - Training Loss: 0.6126\n",
      "Epoch 7/10 - Validation Loss: 0.1679, IoU: 0.0087, Dice: 0.0443\n",
      "Epoch 8/10 - Training Loss: 0.6134\n",
      "Epoch 8/10 - Validation Loss: 0.1303, IoU: 0.0060, Dice: 0.0313\n",
      "Epoch 9/10 - Training Loss: 0.6502\n",
      "Epoch 9/10 - Validation Loss: 0.1235, IoU: 0.0085, Dice: 0.0642\n",
      "Epoch 10/10 - Training Loss: 0.6704\n",
      "Epoch 10/10 - Validation Loss: 0.1965, IoU: 0.0082, Dice: 0.0551\n",
      "Training complete!\n",
      "Trained model saved to: C:/Users/Maverick/Downloads/GFC_Files\\deforestation_unet_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maverick\\AppData\\Local\\Temp\\ipykernel_46804\\3851252404.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained model from: C:/Users/Maverick/Downloads/GFC_Files\\deforestation_unet_model.pth\n",
      "\n",
      "--- Running U-Net Model Evaluation with various thresholds ---\n",
      "\n",
      "Evaluating with prediction threshold: 0.1\n",
      "\n",
      "Starting U-Net model evaluation on 3 file pairs...\n",
      "  Evaluating pair 1: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_070E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_070E.tif\n",
      "  Evaluating pair 2: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_080E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_080E.tif\n",
      "  Evaluating pair 3: Image=Hansen_GFC-2024-v1.12_treecover2000_20N_070E.tif, Mask=Hansen_GFC-2024-v1.12_lossyear_20N_070E.tif\n",
      "U-Net model evaluation complete.\n",
      "  U-Net Aggregated Counts Across All Tiles:\n",
      "  TP=7169678, FP=4334527817, TN=504409015, FN=84082\n",
      "  U-Net Overall Accuracy: 0.1056\n",
      "  U-Net Overall Precision: 0.0017\n",
      "  U-Net Overall Recall: 0.9884\n",
      "  U-Net Overall F1-Score: 0.0033\n",
      "\n",
      "Evaluating with prediction threshold: 0.2\n",
      "\n",
      "Starting U-Net model evaluation on 3 file pairs...\n",
      "  Evaluating pair 1: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_070E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_070E.tif\n",
      "  Evaluating pair 2: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_080E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_080E.tif\n",
      "  Evaluating pair 3: Image=Hansen_GFC-2024-v1.12_treecover2000_20N_070E.tif, Mask=Hansen_GFC-2024-v1.12_lossyear_20N_070E.tif\n",
      "U-Net model evaluation complete.\n",
      "  U-Net Aggregated Counts Across All Tiles:\n",
      "  TP=6468885, FP=1059193716, TN=3779743116, FN=784875\n",
      "  U-Net Overall Accuracy: 0.7813\n",
      "  U-Net Overall Precision: 0.0061\n",
      "  U-Net Overall Recall: 0.8918\n",
      "  U-Net Overall F1-Score: 0.0121\n",
      "\n",
      "Evaluating with prediction threshold: 0.3\n",
      "\n",
      "Starting U-Net model evaluation on 3 file pairs...\n",
      "  Evaluating pair 1: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_070E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_070E.tif\n",
      "  Evaluating pair 2: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_080E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_080E.tif\n",
      "  Evaluating pair 3: Image=Hansen_GFC-2024-v1.12_treecover2000_20N_070E.tif, Mask=Hansen_GFC-2024-v1.12_lossyear_20N_070E.tif\n",
      "U-Net model evaluation complete.\n",
      "  U-Net Aggregated Counts Across All Tiles:\n",
      "  TP=6160753, FP=743454160, TN=4095482672, FN=1093007\n",
      "  U-Net Overall Accuracy: 0.8464\n",
      "  U-Net Overall Precision: 0.0082\n",
      "  U-Net Overall Recall: 0.8493\n",
      "  U-Net Overall F1-Score: 0.0163\n",
      "\n",
      "Evaluating with prediction threshold: 0.4\n",
      "\n",
      "Starting U-Net model evaluation on 3 file pairs...\n",
      "  Evaluating pair 1: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_070E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_070E.tif\n",
      "  Evaluating pair 2: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_080E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_080E.tif\n",
      "  Evaluating pair 3: Image=Hansen_GFC-2024-v1.12_treecover2000_20N_070E.tif, Mask=Hansen_GFC-2024-v1.12_lossyear_20N_070E.tif\n",
      "U-Net model evaluation complete.\n",
      "  U-Net Aggregated Counts Across All Tiles:\n",
      "  TP=5897609, FP=579820421, TN=4259116411, FN=1356151\n",
      "  U-Net Overall Accuracy: 0.8801\n",
      "  U-Net Overall Precision: 0.0101\n",
      "  U-Net Overall Recall: 0.8130\n",
      "  U-Net Overall F1-Score: 0.0199\n",
      "\n",
      "Evaluating with prediction threshold: 0.5\n",
      "\n",
      "Starting U-Net model evaluation on 3 file pairs...\n",
      "  Evaluating pair 1: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_070E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_070E.tif\n",
      "  Evaluating pair 2: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_080E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_080E.tif\n",
      "  Evaluating pair 3: Image=Hansen_GFC-2024-v1.12_treecover2000_20N_070E.tif, Mask=Hansen_GFC-2024-v1.12_lossyear_20N_070E.tif\n",
      "U-Net model evaluation complete.\n",
      "  U-Net Aggregated Counts Across All Tiles:\n",
      "  TP=5511554, FP=458917692, TN=4380019140, FN=1742206\n",
      "  U-Net Overall Accuracy: 0.9049\n",
      "  U-Net Overall Precision: 0.0119\n",
      "  U-Net Overall Recall: 0.7598\n",
      "  U-Net Overall F1-Score: 0.0234\n",
      "\n",
      "Evaluating with prediction threshold: 0.6\n",
      "\n",
      "Starting U-Net model evaluation on 3 file pairs...\n",
      "  Evaluating pair 1: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_070E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_070E.tif\n",
      "  Evaluating pair 2: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_080E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_080E.tif\n",
      "  Evaluating pair 3: Image=Hansen_GFC-2024-v1.12_treecover2000_20N_070E.tif, Mask=Hansen_GFC-2024-v1.12_lossyear_20N_070E.tif\n",
      "U-Net model evaluation complete.\n",
      "  U-Net Aggregated Counts Across All Tiles:\n",
      "  TP=4969210, FP=360068609, TN=4478868223, FN=2284550\n",
      "  U-Net Overall Accuracy: 0.9252\n",
      "  U-Net Overall Precision: 0.0136\n",
      "  U-Net Overall Recall: 0.6851\n",
      "  U-Net Overall F1-Score: 0.0267\n",
      "\n",
      "Evaluating with prediction threshold: 0.7\n",
      "\n",
      "Starting U-Net model evaluation on 3 file pairs...\n",
      "  Evaluating pair 1: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_070E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_070E.tif\n",
      "  Evaluating pair 2: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_080E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_080E.tif\n",
      "  Evaluating pair 3: Image=Hansen_GFC-2024-v1.12_treecover2000_20N_070E.tif, Mask=Hansen_GFC-2024-v1.12_lossyear_20N_070E.tif\n",
      "U-Net model evaluation complete.\n",
      "  U-Net Aggregated Counts Across All Tiles:\n",
      "  TP=4317478, FP=270809943, TN=4568126889, FN=2936282\n",
      "  U-Net Overall Accuracy: 0.9435\n",
      "  U-Net Overall Precision: 0.0157\n",
      "  U-Net Overall Recall: 0.5952\n",
      "  U-Net Overall F1-Score: 0.0306\n",
      "\n",
      "Evaluating with prediction threshold: 0.8\n",
      "\n",
      "Starting U-Net model evaluation on 3 file pairs...\n",
      "  Evaluating pair 1: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_070E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_070E.tif\n",
      "  Evaluating pair 2: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_080E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_080E.tif\n",
      "  Evaluating pair 3: Image=Hansen_GFC-2024-v1.12_treecover2000_20N_070E.tif, Mask=Hansen_GFC-2024-v1.12_lossyear_20N_070E.tif\n",
      "U-Net model evaluation complete.\n",
      "  U-Net Aggregated Counts Across All Tiles:\n",
      "  TP=3335017, FP=171117415, TN=4667819417, FN=3918743\n",
      "  U-Net Overall Accuracy: 0.9639\n",
      "  U-Net Overall Precision: 0.0191\n",
      "  U-Net Overall Recall: 0.4598\n",
      "  U-Net Overall F1-Score: 0.0367\n",
      "\n",
      "Evaluating with prediction threshold: 0.9\n",
      "\n",
      "Starting U-Net model evaluation on 3 file pairs...\n",
      "  Evaluating pair 1: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_070E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_070E.tif\n",
      "  Evaluating pair 2: Image=Hansen_GFC-2019-v1.7_treecover2000_30N_080E.tif, Mask=Hansen_GFC-2019-v1.7_lossyear_30N_080E.tif\n",
      "  Evaluating pair 3: Image=Hansen_GFC-2024-v1.12_treecover2000_20N_070E.tif, Mask=Hansen_GFC-2024-v1.12_lossyear_20N_070E.tif\n",
      "U-Net model evaluation complete.\n",
      "  U-Net Aggregated Counts Across All Tiles:\n",
      "  TP=1626004, FP=68230840, TN=4770705992, FN=5627756\n",
      "  U-Net Overall Accuracy: 0.9848\n",
      "  U-Net Overall Precision: 0.0233\n",
      "  U-Net Overall Recall: 0.2242\n",
      "  U-Net Overall F1-Score: 0.0422\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import math # For math.ceil\n",
    "\n",
    "# Set a device for PyTorch (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ====================================================\n",
    "# 2ï¸âƒ£ Set File Path\n",
    "# ====================================================\n",
    "# Make sure this path points to your folder containing the .tif files\n",
    "data_root_dir = r\"C:/Users/Maverick/Downloads/GFC_Files\"\n",
    "\n",
    "# ====================================================\n",
    "# 3ï¸âƒ£ Define Evaluation Metrics\n",
    "# ====================================================\n",
    "def iou_score(predictions, targets):\n",
    "    \"\"\"Calculates the Intersection over Union (IoU) score.\"\"\"\n",
    "    intersection = (predictions * targets).sum()\n",
    "    union = (predictions + targets - predictions * targets).sum()\n",
    "    return (intersection / union) if union > 0 else 0.0\n",
    "\n",
    "def dice_score(predictions, targets):\n",
    "    \"\"\"Calculates the Dice Coefficient (F1-score for segmentation).\"\"\"\n",
    "    smooth = 1e-6\n",
    "    intersection = (predictions * targets).sum()\n",
    "    return (2. * intersection + smooth) / (predictions.sum() + targets.sum() + smooth)\n",
    "\n",
    "# ====================================================\n",
    "# 4ï¸âƒ£ U-Net Architecture\n",
    "# ====================================================\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => BatchNorm => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # Pad x1 to match the size of x2 if necessary\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1) # Concatenate along the channel dimension\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    \"\"\"Output convolution\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "# ====================================================\n",
    "# 5ï¸âƒ£ Custom Dataset\n",
    "# ====================================================\n",
    "class DeforestationDataset(Dataset):\n",
    "    def __init__(self, data_root_dir, window_size=256, transform=None):\n",
    "        self.data_root_dir = data_root_dir\n",
    "        self.window_size = window_size\n",
    "        self.transform = transform\n",
    "        self.image_mask_pairs = self._find_image_mask_pairs()\n",
    "        self.n_channels = self._get_num_channels()\n",
    "\n",
    "    def _find_image_mask_pairs(self):\n",
    "        all_treecover_files = sorted(glob.glob(os.path.join(self.data_root_dir, \"*_treecover2000_*.tif\")))\n",
    "        all_lossyear_files = sorted(glob.glob(os.path.join(self.data_root_dir, \"*_lossyear_*.tif\")))\n",
    "\n",
    "        image_mask_pairs = []\n",
    "        lossyear_map = {os.path.basename(f).replace('_lossyear_', '_'): f for f in all_lossyear_files}\n",
    "\n",
    "        for treecover_file in all_treecover_files:\n",
    "            basename = os.path.basename(treecover_file)\n",
    "            # Extract the unique identifier part (e.g., _20N_070E.tif)\n",
    "            identifier = basename.replace('_treecover2000_', '_')\n",
    "\n",
    "            if identifier in lossyear_map:\n",
    "                corresponding_loss_file = lossyear_map[identifier]\n",
    "                image_mask_pairs.append((treecover_file, corresponding_loss_file))\n",
    "            else:\n",
    "                print(f\"Warning: Missing corresponding lossyear file for '{treecover_file}'. Skipping.\")\n",
    "        return image_mask_pairs\n",
    "\n",
    "    def _get_num_channels(self):\n",
    "        if not self.image_mask_pairs:\n",
    "            return 1 # Default if no files found\n",
    "        \n",
    "        # Open the first image to determine its number of channels\n",
    "        with rasterio.open(self.image_mask_pairs[0][0]) as src:\n",
    "            return src.count # Number of bands\n",
    "\n",
    "    def __len__(self):\n",
    "        # We need to estimate total number of windows, if we have multiple full tiles\n",
    "        # This is an approximation for potential number of samples\n",
    "        total_windows = 0\n",
    "        for img_path, _ in self.image_mask_pairs:\n",
    "            with rasterio.open(img_path) as src:\n",
    "                # Estimate number of windows per image for rough __len__\n",
    "                # This doesn't account for actual data in windows\n",
    "                num_windows_x = math.ceil(src.width / self.window_size)\n",
    "                num_windows_y = math.ceil(src.height / self.window_size)\n",
    "                total_windows += num_windows_x * num_windows_y\n",
    "        \n",
    "        # A more realistic __len__ is problematic with random windowing\n",
    "        # For practical purposes, we'll return a large number and let DataLoader handle it\n",
    "        # Or, we can simply return the number of distinct image-mask pairs\n",
    "        # For training, it's often better to have a conceptually large dataset\n",
    "        # and let DataLoader pick random windows. So, returning a large constant.\n",
    "        # We'll use a large number representing many potential windows from each file.\n",
    "        return len(self.image_mask_pairs) * 1000 # Each pair can yield many windows\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Select a pair based on idx modulo number of pairs\n",
    "        pair_idx = idx % len(self.image_mask_pairs)\n",
    "        image_path, mask_path = self.image_mask_pairs[pair_idx]\n",
    "\n",
    "        with rasterio.open(image_path) as src_img, \\\n",
    "             rasterio.open(mask_path) as src_mask:\n",
    "\n",
    "            # Get random window coordinates\n",
    "            # Ensure window stays within image bounds\n",
    "            max_x = src_img.width - self.window_size\n",
    "            max_y = src_img.height - self.window_size\n",
    "\n",
    "            if max_x < 0 or max_y < 0: # Image smaller than window size\n",
    "                # Handle cases where the image is smaller than the window size\n",
    "                # e.g., read the whole image and pad if necessary\n",
    "                # For simplicity, we'll just read the available portion and pad later if transform does it\n",
    "                window_x = 0\n",
    "                window_y = 0\n",
    "                current_window_size_x = src_img.width\n",
    "                current_window_size_y = src_img.height\n",
    "            else:\n",
    "                window_x = np.random.randint(0, max_x + 1)\n",
    "                window_y = np.random.randint(0, max_y + 1)\n",
    "                current_window_size_x = self.window_size\n",
    "                current_window_size_y = self.window_size\n",
    "\n",
    "\n",
    "            window = Window(window_x, window_y, current_window_size_x, current_window_size_y)\n",
    "\n",
    "            # Read image and mask data\n",
    "            # Read all bands for the image\n",
    "            image_np = src_img.read(window=window).astype(np.float32)\n",
    "            mask_np = src_mask.read(1, window=window).astype(np.float32) # Assuming mask is single band\n",
    "\n",
    "            # If image_np is 2D (single band), add a channel dimension\n",
    "            if image_np.ndim == 2:\n",
    "                image_np = np.expand_dims(image_np, axis=0) # Add channel dimension (C, H, W)\n",
    "\n",
    "            # Ensure both image and mask are of the specified window_size, pad if necessary\n",
    "            # For smaller images, we need to pad\n",
    "            padded_image = np.zeros((self.n_channels, self.window_size, self.window_size), dtype=np.float32)\n",
    "            padded_mask = np.zeros((self.window_size, self.window_size), dtype=np.float32)\n",
    "\n",
    "            h, w = image_np.shape[1], image_np.shape[2] # Actual height and width read\n",
    "            padded_image[:, :h, :w] = image_np\n",
    "            padded_mask[:mask_np.shape[0], :mask_np.shape[1]] = mask_np\n",
    "\n",
    "            image = torch.from_numpy(padded_image)\n",
    "            mask = torch.from_numpy(padded_mask).unsqueeze(0) # Add channel dimension for mask (1, H, W)\n",
    "\n",
    "            # Normalize image (treecover values are 0-100)\n",
    "            image = image / 100.0 # Normalize to 0-1 range\n",
    "\n",
    "            # Binarize mask: lossyear > 0 means deforestation\n",
    "            mask = (mask > 0).float()\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                mask = self.transform(mask)\n",
    "\n",
    "            return image, mask\n",
    "\n",
    "# ====================================================\n",
    "# 6ï¸âƒ£ Model Training Setup\n",
    "# ====================================================\n",
    "\n",
    "# Data preparation\n",
    "dataset = DeforestationDataset(data_root_dir, window_size=256)\n",
    "total_samples = len(dataset.image_mask_pairs) # Use number of actual distinct file pairs\n",
    "\n",
    "if total_samples > 1:\n",
    "    train_size = int(0.8 * total_samples)\n",
    "    val_size = total_samples - train_size\n",
    "    train_indices, val_indices = random_split(range(total_samples), [train_size, val_size])\n",
    "\n",
    "    # Create datasets that will only use the specified indices\n",
    "    train_dataset_pairs = [dataset.image_mask_pairs[i] for i in train_indices.indices]\n",
    "    val_dataset_pairs = [dataset.image_mask_pairs[i] for i in val_indices.indices]\n",
    "    \n",
    "    # Create new Dataset instances with filtered pairs\n",
    "    train_dataset = DeforestationDataset(data_root_dir, window_size=256)\n",
    "    train_dataset.image_mask_pairs = train_dataset_pairs # Overwrite with specific pairs\n",
    "    \n",
    "    val_dataset = DeforestationDataset(data_root_dir, window_size=256)\n",
    "    val_dataset.image_mask_pairs = val_dataset_pairs # Overwrite with specific pairs\n",
    "\n",
    "    print(f\"Found {total_samples} image-mask pairs.\")\n",
    "    print(f\"Training samples (unique file pairs): {len(train_dataset.image_mask_pairs)}\")\n",
    "    print(f\"Validation samples (unique file pairs): {len(val_dataset.image_mask_pairs)}\")\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "else:\n",
    "    print(f\"Found {total_samples} image-mask pair. Training with limited data.\")\n",
    "    train_dataset = dataset # Use the full dataset for training if only one pair\n",
    "    val_dataset = DeforestationDataset(data_root_dir, window_size=256)\n",
    "    val_dataset.image_mask_pairs = [] # No validation set if only one pair\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "    val_dataloader = None # No validation dataloader\n",
    "\n",
    "# Calculate class weights for BCEWithLogitsLoss\n",
    "print(\"\\nCalculating approximate class weights for loss function...\")\n",
    "num_positive_pixels = 0\n",
    "num_negative_pixels = 0\n",
    "sample_count_for_weight = 0\n",
    "max_samples_for_weight_calc = 100 # Adjust based on your available data and memory\n",
    "\n",
    "temp_train_dataloader_for_weights = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "for images, masks in temp_train_dataloader_for_weights:\n",
    "    masks_np = masks.cpu().numpy()\n",
    "    num_positive_pixels += np.sum(masks_np > 0)\n",
    "    num_negative_pixels += np.sum(masks_np == 0)\n",
    "    sample_count_for_weight += images.shape[0]\n",
    "    if sample_count_for_weight >= max_samples_for_weight_calc:\n",
    "        break\n",
    "\n",
    "if num_positive_pixels == 0:\n",
    "    print(\"Warning: No positive (deforestation) pixels found in sampled batches for weight calculation. Setting weight to 1.0.\")\n",
    "    pos_weight = torch.tensor(1.0).to(device)\n",
    "else:\n",
    "    pos_weight_val = num_negative_pixels / num_positive_pixels\n",
    "    pos_weight = torch.tensor(pos_weight_val).to(device)\n",
    "\n",
    "print(f\"Estimated Positive Pixels: {num_positive_pixels}\")\n",
    "print(f\"Estimated Negative Pixels: {num_negative_pixels}\")\n",
    "print(f\"Calculated `pos_weight` for loss: {pos_weight.item():.2f}\")\n",
    "\n",
    "# Model, Optimizer, Loss\n",
    "n_channels = dataset.n_channels\n",
    "n_classes = 1\n",
    "model = UNet(n_channels=n_channels, n_classes=n_classes).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# ====================================================\n",
    "# 7ï¸âƒ£ Training Loop\n",
    "# ====================================================\n",
    "num_epochs = 10\n",
    "print(f\"\\nStarting training on {len(train_dataset.image_mask_pairs)} unique file pairs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for batch_idx, (images, masks) in enumerate(train_dataloader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_train_loss = running_train_loss / len(train_dataloader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {epoch_train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    if val_dataloader and len(val_dataloader.dataset) > 0:\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        running_val_iou = 0.0\n",
    "        running_val_dice = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_dataloader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                running_val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                preds = torch.sigmoid(outputs)\n",
    "                binary_preds = (preds > 0.5).float()\n",
    "\n",
    "                batch_iou = iou_score(binary_preds, masks)\n",
    "                batch_dice = dice_score(binary_preds, masks)\n",
    "                \n",
    "                running_val_iou += batch_iou * images.size(0)\n",
    "                running_val_dice += batch_dice * images.size(0)\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_dataloader.dataset)\n",
    "        epoch_val_iou = running_val_iou / len(val_dataloader.dataset)\n",
    "        epoch_val_dice = running_val_dice / len(val_dataloader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Validation Loss: {epoch_val_loss:.4f}, IoU: {epoch_val_iou:.4f}, Dice: {epoch_val_dice:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Skipping validation as validation dataset is empty or not created.\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = os.path.join(data_root_dir, \"deforestation_unet_model.pth\")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Trained model saved to: {model_save_path}\")\n",
    "\n",
    "# ====================================================\n",
    "# 8ï¸âƒ£ U-Net Model-Based Evaluation Function\n",
    "# ====================================================\n",
    "def evaluate_deforestation_with_unet(model, data_root_dir, threshold=0.5, tile_size=256):\n",
    "    \"\"\"\n",
    "    Evaluates deforestation detection using a trained U-Net model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained U-Net model.\n",
    "        data_root_dir (str): Root directory containing _treecover2000_ and _lossyear_ files.\n",
    "        threshold (float): Probability threshold for U-Net output to be considered deforestation.\n",
    "        tile_size (int): Size of the square tile (window) for processing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (TP, FP, TN, FN) - Total True Positives, False Positives,\n",
    "               True Negatives, False Negatives across all evaluated tiles.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    total_TP = 0\n",
    "    total_FP = 0\n",
    "    total_TN = 0\n",
    "    total_FN = 0\n",
    "\n",
    "    # Discover all image-mask pairs that the model is trained on\n",
    "    all_treecover_files = sorted(glob.glob(os.path.join(data_root_dir, \"*_treecover2000_*.tif\")))\n",
    "    all_lossyear_files = sorted(glob.glob(os.path.join(data_root_dir, \"*_lossyear_*.tif\")))\n",
    "\n",
    "    evaluation_pairs = []\n",
    "    lossyear_map = {os.path.basename(f).replace('_lossyear_', '_'): f for f in all_lossyear_files}\n",
    "\n",
    "    for treecover_file in all_treecover_files:\n",
    "        basename = os.path.basename(treecover_file)\n",
    "        identifier = basename.replace('_treecover2000_', '_')\n",
    "        if identifier in lossyear_map:\n",
    "            evaluation_pairs.append((treecover_file, lossyear_map[identifier]))\n",
    "\n",
    "    if not evaluation_pairs:\n",
    "        print(\"No _treecover2000_ and _lossyear_ pairs found for U-Net evaluation.\")\n",
    "        return 0, 0, 0, 0\n",
    "\n",
    "    print(f\"\\nStarting U-Net model evaluation on {len(evaluation_pairs)} file pairs...\")\n",
    "\n",
    "    with torch.no_grad(): # No gradient calculation needed during evaluation\n",
    "        for i, (image_path, mask_path) in enumerate(evaluation_pairs):\n",
    "            print(f\"  Evaluating pair {i+1}: Image={os.path.basename(image_path)}, Mask={os.path.basename(mask_path)}\")\n",
    "            \n",
    "            with rasterio.open(image_path) as src_img, \\\n",
    "                 rasterio.open(mask_path) as src_mask:\n",
    "                \n",
    "                width, height = src_img.width, src_img.height\n",
    "                n_channels_img = src_img.count\n",
    "\n",
    "                for row in range(0, height, tile_size):\n",
    "                    for col in range(0, width, tile_size):\n",
    "                        w = Window(col, row, tile_size, tile_size)\n",
    "\n",
    "                        img_tile_np = src_img.read(window=w).astype(np.float32)\n",
    "                        mask_tile_np = src_mask.read(1, window=w).astype(np.float32)\n",
    "\n",
    "                        if img_tile_np.ndim == 2:\n",
    "                            img_tile_np = np.expand_dims(img_tile_np, axis=0)\n",
    "                        \n",
    "                        padded_img = np.zeros((n_channels_img, tile_size, tile_size), dtype=np.float32)\n",
    "                        padded_mask = np.zeros((tile_size, tile_size), dtype=np.float32)\n",
    "\n",
    "                        h_tile, w_tile = img_tile_np.shape[1], img_tile_np.shape[2]\n",
    "                        padded_img[:, :h_tile, :w_tile] = img_tile_np\n",
    "                        padded_mask[:mask_tile_np.shape[0], :mask_tile_np.shape[1]] = mask_tile_np\n",
    "\n",
    "                        input_image = torch.from_numpy(padded_img / 100.0).unsqueeze(0).to(device)\n",
    "                        true_mask = (torch.from_numpy(padded_mask) > 0).float().unsqueeze(0).to(device)\n",
    "\n",
    "                        output_logits = model(input_image)\n",
    "                        predictions = torch.sigmoid(output_logits)\n",
    "                        binary_predictions = (predictions > threshold).float()\n",
    "\n",
    "                        y_true_tile = true_mask.cpu().numpy().flatten()\n",
    "                        y_pred_tile = binary_predictions.cpu().numpy().flatten()\n",
    "\n",
    "                        cm = confusion_matrix(y_true_tile, y_pred_tile, labels=[0,1])\n",
    "                        total_TN += cm[0,0]\n",
    "                        total_FP += cm[0,1]\n",
    "                        total_FN += cm[1,0]\n",
    "                        total_TP += cm[1,1]\n",
    "    \n",
    "    print(\"U-Net model evaluation complete.\")\n",
    "    return total_TP, total_FP, total_TN, total_FN\n",
    "\n",
    "# ====================================================\n",
    "# 9ï¸âƒ£ Run U-Net Model-Based Evaluation\n",
    "# ====================================================\n",
    "\n",
    "# Instantiate the model again (it will load the saved weights)\n",
    "loaded_model = UNet(n_channels=dataset.n_channels, n_classes=1).to(device)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "print(f\"Loaded trained model from: {model_save_path}\")\n",
    "\n",
    "# Define a list of thresholds to experiment with\n",
    "thresholds_to_test = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "print(\"\\n--- Running U-Net Model Evaluation with various thresholds ---\")\n",
    "\n",
    "results_by_threshold = {}\n",
    "\n",
    "for current_threshold in thresholds_to_test:\n",
    "    print(f\"\\nEvaluating with prediction threshold: {current_threshold:.1f}\")\n",
    "    \n",
    "    unet_TP, unet_FP, unet_TN, unet_FN = evaluate_deforestation_with_unet(loaded_model, data_root_dir, threshold=current_threshold)\n",
    "\n",
    "    results_by_threshold[current_threshold] = {\n",
    "        'TP': unet_TP, 'FP': unet_FP, 'TN': unet_TN, 'FN': unet_FN\n",
    "    }\n",
    "\n",
    "    print(f\"  U-Net Aggregated Counts Across All Tiles:\\n  TP={unet_TP}, FP={unet_FP}, TN={unet_TN}, FN={unet_FN}\")\n",
    "\n",
    "    # ====================================================\n",
    "    # ðŸ”Ÿ Compute Metrics from U-Net Aggregated Counts\n",
    "    # ====================================================\n",
    "    if (unet_TP + unet_TN + unet_FP + unet_FN) > 0:\n",
    "        unet_precision = unet_TP / (unet_TP + unet_FP) if (unet_TP + unet_FP) > 0 else 0\n",
    "        unet_recall = unet_TP / (unet_TP + unet_FN) if (unet_TP + unet_FN) > 0 else 0\n",
    "        unet_f1 = 2 * (unet_precision * unet_recall) / (unet_precision + unet_recall) if (unet_precision + unet_recall) > 0 else 0\n",
    "        unet_accuracy = (unet_TP + unet_TN) / (unet_TP + unet_TN + unet_FP + unet_FN)\n",
    "\n",
    "        print(f\"  U-Net Overall Accuracy: {unet_accuracy:.4f}\")\n",
    "        print(f\"  U-Net Overall Precision: {unet_precision:.4f}\")\n",
    "        print(f\"  U-Net Overall Recall: {unet_recall:.4f}\")\n",
    "        print(f\"  U-Net Overall F1-Score: {unet_f1:.4f}\")\n",
    "    else:\n",
    "        print(\"  Cannot compute metrics: No valid data for evaluation.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a250bb07-1db7-4923-a3ed-949e8f93cb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
